{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi , Kevin Mastascusa\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1056660 entries, 0 to 1056659\n",
      "Data columns (total 19 columns):\n",
      " #   Column            Non-Null Count    Dtype  \n",
      "---  ------            --------------    -----  \n",
      " 0   cord_uid          1056660 non-null  object \n",
      " 1   sha               373766 non-null   object \n",
      " 2   source_x          1056660 non-null  object \n",
      " 3   title             1056157 non-null  object \n",
      " 4   doi               656780 non-null   object \n",
      " 5   pmcid             389571 non-null   object \n",
      " 6   pubmed_id         498932 non-null   object \n",
      " 7   license           1056660 non-null  object \n",
      " 8   abstract          821118 non-null   object \n",
      " 9   publish_time      1054846 non-null  object \n",
      " 10  authors           1032791 non-null  object \n",
      " 11  journal           969338 non-null   object \n",
      " 12  mag_id            0 non-null        float64\n",
      " 13  who_covidence_id  482935 non-null   object \n",
      " 14  arxiv_id          14249 non-null    object \n",
      " 15  pdf_json_files    373766 non-null   object \n",
      " 16  pmc_json_files    315742 non-null   object \n",
      " 17  url               686934 non-null   object \n",
      " 18  s2_id             976468 non-null   float64\n",
      "dtypes: float64(2), object(17)\n",
      "memory usage: 153.2+ MB\n",
      "None\n",
      "   cord_uid                                       sha source_x  \\\n",
      "0  ug7v899j  d1aafb70c066a2068b02786f8929fd9c900897fb      PMC   \n",
      "1  02tnwd4m  6b0567729c2143a66d737eb0a2f63f2dce2e5a7d      PMC   \n",
      "2  ejv2xln0  06ced00a5fc04215949aa72528f2eeaae1d58927      PMC   \n",
      "3  2b73a28n  348055649b6b8cf2b9a376498df9bf41f7123605      PMC   \n",
      "4  9785vg6d  5f48792a5fa08bed9f56016f4981ae2ca6031b32      PMC   \n",
      "5  zjufx4fo  b2897e1277f56641193a6db73825f707eed3e4c9      PMC   \n",
      "6  5yhe786e  3bb07ea10432f7738413dff9816809cc90f03f99      PMC   \n",
      "7  8zchiykl  5806726a24dc91de3954001effbdffd7a82d54e2      PMC   \n",
      "8  8qnrcgnk  faaf1022ccfe93b032c5608097a53543ba24aedb      PMC   \n",
      "9  jg13scgo  5b44feca5d6ffaaeb66501fa84cc6dd44d06660a      PMC   \n",
      "\n",
      "                                               title  \\\n",
      "0  Clinical features of culture-proven Mycoplasma...   \n",
      "1  Nitric oxide: a pro-inflammatory mediator in l...   \n",
      "2    Surfactant protein-D and pulmonary host defense   \n",
      "3               Role of endothelin-1 in lung disease   \n",
      "4  Gene expression in epithelial cells in respons...   \n",
      "5  Sequence requirements for RNA strand transfer ...   \n",
      "6  Debate: Transfusing to normal haemoglobin leve...   \n",
      "7  The 21st International Symposium on Intensive ...   \n",
      "8  Heme oxygenase-1 and carbon monoxide in pulmon...   \n",
      "9  Technical Description of RODS: A Real-time Pub...   \n",
      "\n",
      "                        doi      pmcid pubmed_id    license  \\\n",
      "0     10.1186/1471-2334-1-6   PMC35282  11472636      no-cc   \n",
      "1              10.1186/rr14   PMC59543  11667967      no-cc   \n",
      "2              10.1186/rr19   PMC59549  11667972      no-cc   \n",
      "3              10.1186/rr44   PMC59574  11686871      no-cc   \n",
      "4              10.1186/rr61   PMC59580  11686888      no-cc   \n",
      "5  10.1093/emboj/20.24.7220  PMC125340  11742998   green-oa   \n",
      "6             10.1186/cc987  PMC137267  11299062      no-cc   \n",
      "7            10.1186/cc1013  PMC137274  11353930      no-cc   \n",
      "8     10.1186/1465-9921-4-7  PMC193681  12964953      no-cc   \n",
      "9       10.1197/jamia.m1345  PMC212776  12807803  bronze-oa   \n",
      "\n",
      "                                            abstract publish_time  \\\n",
      "0  OBJECTIVE: This retrospective chart review des...   2001-07-04   \n",
      "1  Inflammatory diseases of the respiratory tract...   2000-08-15   \n",
      "2  Surfactant protein-D (SP-D) participates in th...   2000-08-25   \n",
      "3  Endothelin-1 (ET-1) is a 21 amino acid peptide...   2001-02-22   \n",
      "4  Respiratory syncytial virus (RSV) and pneumoni...   2001-05-11   \n",
      "5  Nidovirus subgenomic mRNAs contain a leader se...   2001-12-17   \n",
      "6  Recent evidence suggests that critically ill p...   2001-03-08   \n",
      "7  The 21st International Symposium on Intensive ...   2001-05-02   \n",
      "8  Heme oxygenase-1 (HO-1), an inducible stress p...   2003-08-07   \n",
      "9  This report describes the design and implement...   2003-09-01   \n",
      "\n",
      "                                             authors  \\\n",
      "0                Madani, Tariq A; Al-Ghamdi, Aisha A   \n",
      "1  Vliet, Albert van der; Eiserich, Jason P; Cros...   \n",
      "2                                    Crouch, Erika C   \n",
      "3  Fagan, Karen A; McMurtry, Ivan F; Rodman, David M   \n",
      "4  Domachowske, Joseph B; Bonville, Cynthia A; Ro...   \n",
      "5  Pasternak, Alexander O.; van den Born, Erwin; ...   \n",
      "6    Alvarez, Gonzalo; HÃ©bert, Paul C; Szick, Sharyn   \n",
      "7                      Ball, Jonathan; Venn, Richard   \n",
      "8  Slebos, Dirk-Jan; Ryter, Stefan W; Choi, Augus...   \n",
      "9  Tsui, Fu-Chiang; Espino, Jeremy U.; Dato, Virg...   \n",
      "\n",
      "                                             journal  mag_id who_covidence_id  \\\n",
      "0                                     BMC Infect Dis     NaN              NaN   \n",
      "1                                         Respir Res     NaN              NaN   \n",
      "2                                         Respir Res     NaN              NaN   \n",
      "3                                         Respir Res     NaN              NaN   \n",
      "4                                         Respir Res     NaN              NaN   \n",
      "5                                   The EMBO Journal     NaN              NaN   \n",
      "6                                          Crit Care     NaN              NaN   \n",
      "7                                          Crit Care     NaN              NaN   \n",
      "8                                         Respir Res     NaN              NaN   \n",
      "9  Journal of the American Medical Informatics As...     NaN              NaN   \n",
      "\n",
      "  arxiv_id                                     pdf_json_files  \\\n",
      "0      NaN  document_parses/pdf_json/d1aafb70c066a2068b027...   \n",
      "1      NaN  document_parses/pdf_json/6b0567729c2143a66d737...   \n",
      "2      NaN  document_parses/pdf_json/06ced00a5fc04215949aa...   \n",
      "3      NaN  document_parses/pdf_json/348055649b6b8cf2b9a37...   \n",
      "4      NaN  document_parses/pdf_json/5f48792a5fa08bed9f560...   \n",
      "5      NaN  document_parses/pdf_json/b2897e1277f56641193a6...   \n",
      "6      NaN  document_parses/pdf_json/3bb07ea10432f7738413d...   \n",
      "7      NaN  document_parses/pdf_json/5806726a24dc91de39540...   \n",
      "8      NaN  document_parses/pdf_json/faaf1022ccfe93b032c56...   \n",
      "9      NaN  document_parses/pdf_json/5b44feca5d6ffaaeb6650...   \n",
      "\n",
      "                                pmc_json_files  \\\n",
      "0   document_parses/pmc_json/PMC35282.xml.json   \n",
      "1   document_parses/pmc_json/PMC59543.xml.json   \n",
      "2   document_parses/pmc_json/PMC59549.xml.json   \n",
      "3   document_parses/pmc_json/PMC59574.xml.json   \n",
      "4   document_parses/pmc_json/PMC59580.xml.json   \n",
      "5  document_parses/pmc_json/PMC125340.xml.json   \n",
      "6  document_parses/pmc_json/PMC137267.xml.json   \n",
      "7  document_parses/pmc_json/PMC137274.xml.json   \n",
      "8  document_parses/pmc_json/PMC193681.xml.json   \n",
      "9  document_parses/pmc_json/PMC212776.xml.json   \n",
      "\n",
      "                                                 url  s2_id  \n",
      "0  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3...    NaN  \n",
      "1  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...    NaN  \n",
      "2  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...    NaN  \n",
      "3  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...    NaN  \n",
      "4  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...    NaN  \n",
      "5  http://europepmc.org/articles/pmc125340?pdf=re...    NaN  \n",
      "6  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...    NaN  \n",
      "7  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...    NaN  \n",
      "8  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...    NaN  \n",
      "9  https://academic.oup.com/jamia/article-pdf/10/...    NaN  \n",
      "        cord_uid                                       sha source_x    title  \\\n",
      "count    1056660                                    373766  1056660  1056157   \n",
      "unique    970836                                    373719       49   850366   \n",
      "top     kgpo6psq  31bc0fb718edaab9e33f678909710f62c40abebc      WHO    Reply   \n",
      "freq         192                                         3   450459      251   \n",
      "mean         NaN                                       NaN      NaN      NaN   \n",
      "std          NaN                                       NaN      NaN      NaN   \n",
      "min          NaN                                       NaN      NaN      NaN   \n",
      "25%          NaN                                       NaN      NaN      NaN   \n",
      "50%          NaN                                       NaN      NaN      NaN   \n",
      "75%          NaN                                       NaN      NaN      NaN   \n",
      "max          NaN                                       NaN      NaN      NaN   \n",
      "\n",
      "                                    doi     pmcid pubmed_id  license  \\\n",
      "count                            656780    389571    498932  1056660   \n",
      "unique                           655525    389571    498449       18   \n",
      "top     10.1016/j.scitotenv.2020.139397  PMC35282  35087663      unk   \n",
      "freq                                  9         1         5   601506   \n",
      "mean                                NaN       NaN       NaN      NaN   \n",
      "std                                 NaN       NaN       NaN      NaN   \n",
      "min                                 NaN       NaN       NaN      NaN   \n",
      "25%                                 NaN       NaN       NaN      NaN   \n",
      "50%                                 NaN       NaN       NaN      NaN   \n",
      "75%                                 NaN       NaN       NaN      NaN   \n",
      "max                                 NaN       NaN       NaN      NaN   \n",
      "\n",
      "                   abstract publish_time     authors   journal  mag_id  \\\n",
      "count                821118      1054846     1032791    969338     0.0   \n",
      "unique               730713         8056      796659     54993     NaN   \n",
      "top     [Figure: see text].         2021  Anonymous,  PLoS One     NaN   \n",
      "freq                    206       233709        3904      9953     NaN   \n",
      "mean                    NaN          NaN         NaN       NaN     NaN   \n",
      "std                     NaN          NaN         NaN       NaN     NaN   \n",
      "min                     NaN          NaN         NaN       NaN     NaN   \n",
      "25%                     NaN          NaN         NaN       NaN     NaN   \n",
      "50%                     NaN          NaN         NaN       NaN     NaN   \n",
      "75%                     NaN          NaN         NaN       NaN     NaN   \n",
      "max                     NaN          NaN         NaN       NaN     NaN   \n",
      "\n",
      "         who_covidence_id    arxiv_id  \\\n",
      "count              482935       14249   \n",
      "unique             482935       14249   \n",
      "top     #covidwho-1638294  2110.00181   \n",
      "freq                    1           1   \n",
      "mean                  NaN         NaN   \n",
      "std                   NaN         NaN   \n",
      "min                   NaN         NaN   \n",
      "25%                   NaN         NaN   \n",
      "50%                   NaN         NaN   \n",
      "75%                   NaN         NaN   \n",
      "max                   NaN         NaN   \n",
      "\n",
      "                                           pdf_json_files  \\\n",
      "count                                              373766   \n",
      "unique                                             373719   \n",
      "top     document_parses/pdf_json/31bc0fb718edaab9e33f6...   \n",
      "freq                                                    3   \n",
      "mean                                                  NaN   \n",
      "std                                                   NaN   \n",
      "min                                                   NaN   \n",
      "25%                                                   NaN   \n",
      "50%                                                   NaN   \n",
      "75%                                                   NaN   \n",
      "max                                                   NaN   \n",
      "\n",
      "                                    pmc_json_files  \\\n",
      "count                                       315742   \n",
      "unique                                      315742   \n",
      "top     document_parses/pmc_json/PMC35282.xml.json   \n",
      "freq                                             1   \n",
      "mean                                           NaN   \n",
      "std                                            NaN   \n",
      "min                                            NaN   \n",
      "25%                                            NaN   \n",
      "50%                                            NaN   \n",
      "75%                                            NaN   \n",
      "max                                            NaN   \n",
      "\n",
      "                                                      url         s2_id  \n",
      "count                                              686934  9.764680e+05  \n",
      "unique                                             686934           NaN  \n",
      "top     https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3...           NaN  \n",
      "freq                                                    1           NaN  \n",
      "mean                                                  NaN  2.175871e+08  \n",
      "std                                                   NaN  5.312281e+07  \n",
      "min                                                   NaN  9.600000e+01  \n",
      "25%                                                   NaN  2.211411e+08  \n",
      "50%                                                   NaN  2.320829e+08  \n",
      "75%                                                   NaN  2.373948e+08  \n",
      "max                                                   NaN  2.491936e+08  \n",
      "cord_uid                  0\n",
      "sha                  682894\n",
      "source_x                  0\n",
      "title                   503\n",
      "doi                  399880\n",
      "pmcid                667089\n",
      "pubmed_id            557728\n",
      "license                   0\n",
      "abstract             235542\n",
      "publish_time           1814\n",
      "authors               23869\n",
      "journal               87322\n",
      "mag_id              1056660\n",
      "who_covidence_id     573725\n",
      "arxiv_id            1042411\n",
      "pdf_json_files       682894\n",
      "pmc_json_files       740918\n",
      "url                  369726\n",
      "s2_id                 80192\n",
      "dtype: int64\n",
      "cord_uid            970836\n",
      "sha                 373719\n",
      "source_x                49\n",
      "title               850366\n",
      "doi                 655525\n",
      "pmcid               389571\n",
      "pubmed_id           498449\n",
      "license                 18\n",
      "abstract            730713\n",
      "publish_time          8056\n",
      "authors             796659\n",
      "journal              54993\n",
      "mag_id                   0\n",
      "who_covidence_id    482935\n",
      "arxiv_id             14249\n",
      "pdf_json_files      373719\n",
      "pmc_json_files      315742\n",
      "url                 686934\n",
      "s2_id               678262\n",
      "dtype: int64\n",
      "====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n",
      "Data Preprocessing\n",
      "====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n",
      "Drop the columns that are not required:\n",
      "Check the structure of the data:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1056660 entries, 0 to 1056659\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   title   1056157 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 8.1+ MB\n",
      "None\n",
      "Print the first 10 rows of the data:\n",
      "                                               title\n",
      "0  Clinical features of culture-proven Mycoplasma...\n",
      "1  Nitric oxide: a pro-inflammatory mediator in l...\n",
      "2    Surfactant protein-D and pulmonary host defense\n",
      "3               Role of endothelin-1 in lung disease\n",
      "4  Gene expression in epithelial cells in respons...\n",
      "5  Sequence requirements for RNA strand transfer ...\n",
      "6  Debate: Transfusing to normal haemoglobin leve...\n",
      "7  The 21st International Symposium on Intensive ...\n",
      "8  Heme oxygenase-1 and carbon monoxide in pulmon...\n",
      "9  Technical Description of RODS: A Real-time Pub...\n",
      "Print a summary of the data:\n",
      "          title\n",
      "count   1056157\n",
      "unique   850366\n",
      "top       Reply\n",
      "freq        251\n",
      "Number of missing values in each column:\n",
      "title    503\n",
      "dtype: int64\n",
      "Number of unique values in each column:\n",
      "title    850366\n",
      "dtype: int64\n",
      "Drop the rows with missing values:\n",
      "Number of missing values in each column:\n",
      "title    0\n",
      "dtype: int64\n",
      "Number of unique values in each column:\n",
      "title    850366\n",
      "dtype: int64\n",
      "MARKER 1\n",
      " ------------------------------------ \n",
      " ------------------------------------ \n",
      "Add target column for classification:\n",
      "Print the first 10 rows of the data:\n",
      "                                               title  target\n",
      "0  Clinical features of culture-proven Mycoplasma...       0\n",
      "1  Nitric oxide: a pro-inflammatory mediator in l...       0\n",
      "2    Surfactant protein-D and pulmonary host defense       0\n",
      "3               Role of endothelin-1 in lung disease       0\n",
      "4  Gene expression in epithelial cells in respons...       0\n",
      "5  Sequence requirements for RNA strand transfer ...       0\n",
      "6  Debate: Transfusing to normal haemoglobin leve...       0\n",
      "7  The 21st International Symposium on Intensive ...       0\n",
      "8  Heme oxygenase-1 and carbon monoxide in pulmon...       0\n",
      "9  Technical Description of RODS: A Real-time Pub...       0\n",
      "Number of missing values in each column:\n",
      "title     0\n",
      "target    0\n",
      "dtype: int64\n",
      "Number of unique values in each column:\n",
      "title     850366\n",
      "target         2\n",
      "dtype: int64\n",
      "Class distribution:\n",
      "0    612031\n",
      "1    444126\n",
      "Name: target, dtype: int64\n",
      "Class distribution plot:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------------------------------------ \n",
      " ------------------------------------ \n",
      "MARKER 2\n",
      " ------------------------------------ \n",
      " ------------------------------------ \n",
      "Data Preprocessing:\n",
      "NUMPY ARRAY CONVERSION:\n",
      "Convert the dataframe to numpy array:\n",
      "Print the numpy array:\n",
      "[['Clinical features of culture-proven Mycoplasma pneumoniae infections at King Abdulaziz University Hospital, Jeddah, Saudi Arabia'\n",
      "  0]\n",
      " ['Nitric oxide: a pro-inflammatory mediator in lung disease?' 0]\n",
      " ['Surfactant protein-D and pulmonary host defense' 0]\n",
      " ...\n",
      " ['A Patient With Bilateral Conjunctivitis Positive for SARS-CoV-2 RNA in a Conjunctival Sample'\n",
      "  0]\n",
      " ['Incidental lowering of otitis-media complaints in otitis-prone children during COVID-19 pandemic: not all evil comes to hurt'\n",
      "  1]\n",
      " ['Hospital variation in admissions to neonatal intensive care units by diagnosis severity and category'\n",
      "  0]]\n",
      "Print the shape of the numpy array:\n",
      "(1056157, 2)\n",
      "Print the type of the numpy array:\n",
      "<class 'numpy.ndarray'>\n",
      "Print the first row of the numpy array:\n",
      "['Clinical features of culture-proven Mycoplasma pneumoniae infections at King Abdulaziz University Hospital, Jeddah, Saudi Arabia'\n",
      " 0]\n",
      "Print the second row of the numpy array:\n",
      "['Nitric oxide: a pro-inflammatory mediator in lung disease?' 0]\n",
      "Print the third row of the numpy array:\n",
      "['Surfactant protein-D and pulmonary host defense' 0]\n",
      "Print the fourth row of the numpy array:\n",
      "['Role of endothelin-1 in lung disease' 0]\n",
      "Print the fifth row of the numpy array:\n",
      "['Gene expression in epithelial cells in response to pneumovirus infection'\n",
      " 0]\n",
      "Print the first column of the numpy array:\n",
      "['Clinical features of culture-proven Mycoplasma pneumoniae infections at King Abdulaziz University Hospital, Jeddah, Saudi Arabia'\n",
      " 'Nitric oxide: a pro-inflammatory mediator in lung disease?'\n",
      " 'Surfactant protein-D and pulmonary host defense' ...\n",
      " 'A Patient With Bilateral Conjunctivitis Positive for SARS-CoV-2 RNA in a Conjunctival Sample'\n",
      " 'Incidental lowering of otitis-media complaints in otitis-prone children during COVID-19 pandemic: not all evil comes to hurt'\n",
      " 'Hospital variation in admissions to neonatal intensive care units by diagnosis severity and category']\n",
      "Print the second column of the numpy array:\n",
      "[0 0 0 ... 0 1 0]\n",
      "Print the first row and first column of the numpy array:\n",
      "Clinical features of culture-proven Mycoplasma pneumoniae infections at King Abdulaziz University Hospital, Jeddah, Saudi Arabia\n",
      "Print the first row and second column of the numpy array:\n",
      "0\n",
      "Export as csv file:\n",
      "['title,target\\n', '\"Clinical features of culture-proven Mycoplasma pneumoniae infections at King Abdulaziz University Hospital, Jeddah, Saudi Arabia\",0\\n', 'Nitric oxide: a pro-inflammatory mediator in lung disease?,0\\n', 'Surfactant protein-D and pulmonary host defense,0\\n', 'Role of endothelin-1 in lung disease,0\\n']\n",
      " ------------------------------------ \n",
      " ------------------------------------ \n",
      "text cleaning:\n",
      "Convert to lower case:\n",
      "Remove punctuation:\n",
      "Remove numbers:\n",
      "Remove whitespaces:\n",
      "Remove stopwords:\n",
      "Remove short words:\n",
      "Lemmatization:\n",
      "Print the first 5 rows of the data:\n",
      "                                               title  target\n",
      "0  clinical feature cultureproven mycoplasma pneu...       0\n",
      "1  nitric oxide proinflammatory mediator lung dis...       0\n",
      "2         surfactant proteind pulmonary host defense       0\n",
      "3                       role endothelin lung disease       0\n",
      "4  gene expression epithelial cell response pneum...       0\n",
      "Number of missing values in each column:\n",
      "title     0\n",
      "target    0\n",
      "dtype: int64\n",
      "Number of unique values in each column:\n",
      "title     759694\n",
      "target         2\n",
      "dtype: int64\n",
      "['title,target\\n', 'clinical feature cultureproven mycoplasma pneumoniae infection king abdulaziz university hospital jeddah saudi arabia,0\\n', 'nitric oxide proinflammatory mediator lung disease,0\\n', 'surfactant proteind pulmonary host defense,0\\n', 'role endothelin lung disease,0\\n']\n",
      " ------------------------------------ \n",
      "TF Text Vectorizer:\n",
      "Print the shape of the TF Binary vectorizer:\n",
      "(1056157, 1000)\n",
      "Print the type of the TF Binary vectorizer:\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "Print the TF Binary vectorizer:\n",
      "  (0, 50)\t1\n",
      "  (0, 137)\t1\n",
      "  (0, 324)\t1\n",
      "  (0, 383)\t1\n",
      "  (0, 427)\t1\n",
      "  (0, 787)\t1\n",
      "  (0, 951)\t1\n",
      "  (1, 237)\t1\n",
      "  (1, 499)\t1\n",
      "  (2, 386)\t1\n",
      "  (2, 712)\t1\n",
      "  (3, 237)\t1\n",
      "  (3, 499)\t1\n",
      "  (3, 776)\t1\n",
      "  (4, 114)\t1\n",
      "  (4, 292)\t1\n",
      "  (4, 315)\t1\n",
      "  (4, 351)\t1\n",
      "  (4, 427)\t1\n",
      "  (4, 768)\t1\n",
      "  (5, 805)\t1\n",
      "  (5, 887)\t1\n",
      "  (5, 924)\t1\n",
      "  (6, 407)\t1\n",
      "  (6, 483)\t1\n",
      "  :\t:\n",
      "  (1056151, 749)\t1\n",
      "  (1056151, 884)\t1\n",
      "  (1056151, 898)\t1\n",
      "  (1056152, 183)\t1\n",
      "  (1056152, 580)\t1\n",
      "  (1056152, 735)\t1\n",
      "  (1056153, 348)\t1\n",
      "  (1056153, 412)\t1\n",
      "  (1056153, 935)\t1\n",
      "  (1056154, 624)\t1\n",
      "  (1056154, 658)\t1\n",
      "  (1056154, 783)\t1\n",
      "  (1056154, 785)\t1\n",
      "  (1056155, 127)\t1\n",
      "  (1056155, 193)\t1\n",
      "  (1056155, 613)\t1\n",
      "  (1056156, 17)\t1\n",
      "  (1056156, 109)\t1\n",
      "  (1056156, 226)\t1\n",
      "  (1056156, 383)\t1\n",
      "  (1056156, 446)\t1\n",
      "  (1056156, 568)\t1\n",
      "  (1056156, 814)\t1\n",
      "  (1056156, 948)\t1\n",
      "  (1056156, 965)\t1\n",
      "Print the TF Binary vectorizer to array:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Print the TF Binary vectorizer to array shape:\n",
      "(1056157, 1000)\n",
      "Print the TF Binary vectorizer to array type:\n",
      "<class 'numpy.ndarray'>\n",
      "Print the TF Binary vectorizer to array first row:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0]\n",
      "Print the TF Binary vectorizer to array first row shape:\n",
      "(1000,)\n",
      "Print the TF Binary vectorizer to array first row type:\n",
      "<class 'numpy.ndarray'>\n",
      "TF-IDF Text Vectorizer:\n",
      "Print the shape of the TF-IDF vectorizer:\n",
      "(1056157, 1000)\n",
      "Print the type of the TF-IDF vectorizer:\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "Print the TF-IDF vectorizer:\n",
      "  (0, 951)\t0.3938293261930267\n",
      "  (0, 786)\t0.45308250376886283\n",
      "  (0, 427)\t0.25739840126780433\n",
      "  (0, 384)\t0.32901435077890834\n",
      "  (0, 325)\t0.4054435230495919\n",
      "  (0, 138)\t0.2868477573850907\n",
      "  (0, 51)\t0.46738018819316246\n",
      "  (1, 498)\t0.8248279157344968\n",
      "  (1, 238)\t0.5653838602446005\n",
      "  (2, 711)\t0.6675764385153931\n",
      "  (2, 387)\t0.7445412673177382\n",
      "  (3, 775)\t0.5905830536949489\n",
      "  (3, 498)\t0.6656170435817038\n",
      "  (3, 238)\t0.4562516936756521\n",
      "  (4, 767)\t0.325382872733194\n",
      "  (4, 427)\t0.2809639817816664\n",
      "  (4, 352)\t0.4426233425571942\n",
      "  (4, 316)\t0.4499301004407103\n",
      "  (4, 293)\t0.5411521622902717\n",
      "  (4, 115)\t0.352117942829896\n",
      "  (5, 923)\t0.5976282814749584\n",
      "  (5, 886)\t0.5580178892626383\n",
      "  (5, 803)\t0.5757225655158549\n",
      "  (6, 600)\t0.39030891479871493\n",
      "  (6, 575)\t0.5766129547781752\n",
      "  :\t:\n",
      "  (1056151, 600)\t0.32895629895212225\n",
      "  (1056151, 523)\t0.3964595399818659\n",
      "  (1056151, 194)\t0.12417896846118726\n",
      "  (1056151, 131)\t0.43579369257698924\n",
      "  (1056152, 734)\t0.7578541503017946\n",
      "  (1056152, 578)\t0.5141602501261829\n",
      "  (1056152, 184)\t0.4016171361639414\n",
      "  (1056153, 934)\t0.736803085203963\n",
      "  (1056153, 349)\t0.6761073980026557\n",
      "  (1056154, 784)\t0.3395577282389928\n",
      "  (1056154, 782)\t0.6303605993840131\n",
      "  (1056154, 657)\t0.6221607476523217\n",
      "  (1056154, 622)\t0.31664186081139983\n",
      "  (1056155, 611)\t0.49849611748306205\n",
      "  (1056155, 194)\t0.30010853749892863\n",
      "  (1056155, 128)\t0.8132874562997804\n",
      "  (1056156, 965)\t0.38843357839976134\n",
      "  (1056156, 948)\t0.33700391383303224\n",
      "  (1056156, 812)\t0.3195599104650355\n",
      "  (1056156, 567)\t0.3956424825503281\n",
      "  (1056156, 446)\t0.3451778583765676\n",
      "  (1056156, 384)\t0.2744590216258018\n",
      "  (1056156, 227)\t0.30839143832538213\n",
      "  (1056156, 110)\t0.23292868326787514\n",
      "  (1056156, 17)\t0.3647734196336631\n",
      "Print the TF-IDF vectorizer to array:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Print the TF-IDF vectorizer to array shape:\n",
      "(1056157, 1000)\n",
      "Print the TF-IDF vectorizer to array type:\n",
      "<class 'numpy.ndarray'>\n",
      "Print the TF-IDF vectorizer to array first row:\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.46738019 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.28684776 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.40544352 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.32901435 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.2573984  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.4530825  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.39382933 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "Print the TF-IDF vectorizer to array first row shape:\n",
      "(1000,)\n",
      " ------------------------------------ \n",
      " ------------------------------------ \n",
      " ------------------------------------ \n",
      "Split the data into training and testing sets:\n",
      "Print the shape of the training set:\n",
      "(844925, 1000)\n",
      "Print the shape of the testing set:\n",
      "(211232, 1000)\n",
      "Print the shape of the training labels:\n",
      "(844925,)\n",
      "Print the shape of the testing labels:\n",
      "(211232,)\n",
      "Train the model:\n",
      "Predict the labels:\n",
      "Print the predicted labels:\n",
      "[1 0 1 ... 0 1 1]\n",
      "Evaluate the model:\n",
      "Accuracy:\n",
      "0.794893766096046\n",
      "Precision, Recall, F1-Score:\n",
      "(0.7906897134667983, 0.7866048629823408, 0.7883248590231003, None)\n",
      "Confusion Matrix:\n",
      "[[102559  19608]\n",
      " [ 23717  65348]]\n",
      " ------------------------------------ \n",
      "KNN Classifier:\n",
      "==================================================\n",
      "Take a small sample of the data:\n",
      "Print the shape of the sample:\n",
      "(1000, 2)\n",
      "TF-IDF Text Vectorizer:\n",
      "Print the shape of the TF-IDF vectorizer:\n",
      "(1000, 1000)\n",
      "Print the type of the TF-IDF vectorizer:\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "Print the TF-IDF vectorizer:\n",
      "  (0, 930)\t0.5223048050592047\n",
      "  (0, 712)\t0.5223048050592047\n",
      "  (0, 594)\t0.2784427624281276\n",
      "  (0, 296)\t0.5890533744095889\n",
      "  (0, 171)\t0.17286159600154732\n",
      "  (1, 952)\t0.5508649598980113\n",
      "  (1, 635)\t0.45807987949302104\n",
      "  (1, 619)\t0.403133452753087\n",
      "  (1, 361)\t0.3279790983002038\n",
      "  (1, 277)\t0.4654285662803492\n",
      "  (2, 871)\t0.5123141812750858\n",
      "  (2, 651)\t0.5123141812750858\n",
      "  (2, 540)\t0.42030256009898087\n",
      "  (2, 181)\t0.5462729329768405\n",
      "  (3, 969)\t0.5636057620646413\n",
      "  (3, 890)\t0.4162984905672204\n",
      "  (3, 608)\t0.2641315453774545\n",
      "  (3, 213)\t0.503710918727204\n",
      "  (3, 171)\t0.1551122115774975\n",
      "  (3, 9)\t0.4018633482043871\n",
      "  (4, 875)\t0.23653488145235566\n",
      "  (4, 608)\t0.19675950372069342\n",
      "  (4, 585)\t0.31954999812067003\n",
      "  (4, 543)\t0.41984682245919913\n",
      "  (4, 302)\t0.38382469349276593\n",
      "  :\t:\n",
      "  (996, 267)\t0.43143487015386384\n",
      "  (996, 206)\t0.228363625108895\n",
      "  (996, 165)\t0.24489932403316297\n",
      "  (996, 94)\t0.297112332161632\n",
      "  (997, 968)\t0.32483123128697605\n",
      "  (997, 861)\t0.39062650687596484\n",
      "  (997, 795)\t0.2135028233839735\n",
      "  (997, 693)\t0.32483123128697605\n",
      "  (997, 653)\t0.39062650687596484\n",
      "  (997, 301)\t0.40785562531402975\n",
      "  (997, 220)\t0.32483123128697605\n",
      "  (997, 63)\t0.40785562531402975\n",
      "  (998, 830)\t0.4389016141357857\n",
      "  (998, 428)\t0.5127503720922321\n",
      "  (998, 391)\t0.474289204940446\n",
      "  (998, 385)\t0.35618471642427474\n",
      "  (998, 133)\t0.4389016141357857\n",
      "  (999, 963)\t0.30043168945763854\n",
      "  (999, 485)\t0.3290206550851362\n",
      "  (999, 374)\t0.4362548871271673\n",
      "  (999, 193)\t0.4035316107575826\n",
      "  (999, 171)\t0.11499160191552907\n",
      "  (999, 20)\t0.29317190115147945\n",
      "  (999, 15)\t0.41782609357133144\n",
      "  (999, 12)\t0.41782609357133144\n",
      "Print the TF-IDF vectorizer to array:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Print the TF-IDF vectorizer to array shape:\n",
      "(1000, 1000)\n",
      "Print the TF-IDF vectorizer to array type:\n",
      "<class 'numpy.ndarray'>\n",
      "Print the TF-IDF vectorizer to array first row:\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.1728616  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.58905337 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.27844276 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.52230481 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.52230481 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "Print the TF-IDF vectorizer to array first row shape:\n",
      "(1000,)\n",
      " ------------------------------------ \n",
      "Split the data into training and testing sets:\n",
      "Print the shape of the training set:\n",
      "(800, 1000)\n",
      "Print the shape of the testing set:\n",
      "(200, 1000)\n",
      "Print the shape of the training labels:\n",
      "(800,)\n",
      "Print the shape of the testing labels:\n",
      "(200,)\n",
      "Train the model:\n",
      "Predict the labels:\n",
      "Print the predicted labels:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Evaluate the model:\n",
      "Accuracy:\n",
      "0.56\n",
      "Precision, Recall, F1-Score:\n",
      "(0.778894472361809, 0.5056179775280899, 0.3691756272401434, None)\n",
      "Confusion Matrix:\n",
      "[[111   0]\n",
      " [ 88   1]]\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      1.00      0.72       111\n",
      "           1       1.00      0.01      0.02        89\n",
      "\n",
      "    accuracy                           0.56       200\n",
      "   macro avg       0.78      0.51      0.37       200\n",
      "weighted avg       0.75      0.56      0.41       200\n",
      "\n",
      "mean_squared_error:\n",
      "0.44\n",
      "mean_absolute_error:\n",
      "0.44\n",
      "r2_score:\n",
      "-0.7815568377366138\n",
      "explained_variance_score:\n",
      "0.00232817086749626\n",
      "Save classification report to csv file:\n",
      "Save confusion matrix to csv file:\n",
      " ------------------------------------ \n",
      "Naive Bayes Classifier:\n",
      "==================================================\n",
      "Take a small sample of the data:\n",
      "Print the shape of the sample:\n",
      "(1000, 2)\n",
      "TF-IDF Text Vectorizer:\n",
      "Print the shape of the TF-IDF vectorizer:\n",
      "(1000, 1000)\n",
      "Print the type of the TF-IDF vectorizer:\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "Print the TF-IDF vectorizer:\n",
      "  (0, 930)\t0.5223048050592047\n",
      "  (0, 712)\t0.5223048050592047\n",
      "  (0, 594)\t0.2784427624281276\n",
      "  (0, 296)\t0.5890533744095889\n",
      "  (0, 171)\t0.17286159600154732\n",
      "  (1, 952)\t0.5508649598980113\n",
      "  (1, 635)\t0.45807987949302104\n",
      "  (1, 619)\t0.403133452753087\n",
      "  (1, 361)\t0.3279790983002038\n",
      "  (1, 277)\t0.4654285662803492\n",
      "  (2, 871)\t0.5123141812750858\n",
      "  (2, 651)\t0.5123141812750858\n",
      "  (2, 540)\t0.42030256009898087\n",
      "  (2, 181)\t0.5462729329768405\n",
      "  (3, 969)\t0.5636057620646413\n",
      "  (3, 890)\t0.4162984905672204\n",
      "  (3, 608)\t0.2641315453774545\n",
      "  (3, 213)\t0.503710918727204\n",
      "  (3, 171)\t0.1551122115774975\n",
      "  (3, 9)\t0.4018633482043871\n",
      "  (4, 875)\t0.23653488145235566\n",
      "  (4, 608)\t0.19675950372069342\n",
      "  (4, 585)\t0.31954999812067003\n",
      "  (4, 543)\t0.41984682245919913\n",
      "  (4, 302)\t0.38382469349276593\n",
      "  :\t:\n",
      "  (996, 267)\t0.43143487015386384\n",
      "  (996, 206)\t0.228363625108895\n",
      "  (996, 165)\t0.24489932403316297\n",
      "  (996, 94)\t0.297112332161632\n",
      "  (997, 968)\t0.32483123128697605\n",
      "  (997, 861)\t0.39062650687596484\n",
      "  (997, 795)\t0.2135028233839735\n",
      "  (997, 693)\t0.32483123128697605\n",
      "  (997, 653)\t0.39062650687596484\n",
      "  (997, 301)\t0.40785562531402975\n",
      "  (997, 220)\t0.32483123128697605\n",
      "  (997, 63)\t0.40785562531402975\n",
      "  (998, 830)\t0.4389016141357857\n",
      "  (998, 428)\t0.5127503720922321\n",
      "  (998, 391)\t0.474289204940446\n",
      "  (998, 385)\t0.35618471642427474\n",
      "  (998, 133)\t0.4389016141357857\n",
      "  (999, 963)\t0.30043168945763854\n",
      "  (999, 485)\t0.3290206550851362\n",
      "  (999, 374)\t0.4362548871271673\n",
      "  (999, 193)\t0.4035316107575826\n",
      "  (999, 171)\t0.11499160191552907\n",
      "  (999, 20)\t0.29317190115147945\n",
      "  (999, 15)\t0.41782609357133144\n",
      "  (999, 12)\t0.41782609357133144\n",
      "Split the data into training and testing sets:\n",
      "Print the shape of the training set:\n",
      "(800, 1000)\n",
      "Print the shape of the testing set:\n",
      "(200, 1000)\n",
      "Print the shape of the training labels:\n",
      "(800,)\n",
      "Print the shape of the testing labels:\n",
      "(200,)\n",
      "Train the model:\n",
      "Predict the labels:\n",
      "Print the predicted labels:\n",
      "[0 0 0 1 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1\n",
      " 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 0\n",
      " 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1\n",
      " 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1 0 1 0\n",
      " 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1\n",
      " 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1]\n",
      "Evaluate the model:\n",
      "Accuracy:\n",
      "0.72\n",
      "Precision, Recall, F1-Score:\n",
      "(0.7173980703392469, 0.7121166109930155, 0.7135549872122762, None)\n",
      "Confusion Matrix:\n",
      "[[87 24]\n",
      " [32 57]]\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.78      0.76       111\n",
      "           1       0.70      0.64      0.67        89\n",
      "\n",
      "    accuracy                           0.72       200\n",
      "   macro avg       0.72      0.71      0.71       200\n",
      "weighted avg       0.72      0.72      0.72       200\n",
      "\n",
      "mean_squared_error:\n",
      "0.28\n",
      "mean_absolute_error:\n",
      "0.28\n",
      "r2_score:\n",
      "-0.1337179876505723\n",
      "explained_variance_score:\n",
      "-0.12723959914971172\n",
      " ------------------------------------ \n",
      "Save classification report to csv file:\n",
      "Save confusion matrix to csv file:\n",
      "Decision Tree Classifier:\n",
      "==================================================\n",
      "Take a small sample of the data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Text Vectorizer:\n",
      "Print the shape of the TF-IDF vectorizer:\n",
      "(1000, 1000)\n",
      "Print the type of the TF-IDF vectorizer:\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "Print the TF-IDF vectorizer:\n",
      "  (0, 930)\t0.5223048050592047\n",
      "  (0, 712)\t0.5223048050592047\n",
      "  (0, 594)\t0.2784427624281276\n",
      "  (0, 296)\t0.5890533744095889\n",
      "  (0, 171)\t0.17286159600154732\n",
      "  (1, 952)\t0.5508649598980113\n",
      "  (1, 635)\t0.45807987949302104\n",
      "  (1, 619)\t0.403133452753087\n",
      "  (1, 361)\t0.3279790983002038\n",
      "  (1, 277)\t0.4654285662803492\n",
      "  (2, 871)\t0.5123141812750858\n",
      "  (2, 651)\t0.5123141812750858\n",
      "  (2, 540)\t0.42030256009898087\n",
      "  (2, 181)\t0.5462729329768405\n",
      "  (3, 969)\t0.5636057620646413\n",
      "  (3, 890)\t0.4162984905672204\n",
      "  (3, 608)\t0.2641315453774545\n",
      "  (3, 213)\t0.503710918727204\n",
      "  (3, 171)\t0.1551122115774975\n",
      "  (3, 9)\t0.4018633482043871\n",
      "  (4, 875)\t0.23653488145235566\n",
      "  (4, 608)\t0.19675950372069342\n",
      "  (4, 585)\t0.31954999812067003\n",
      "  (4, 543)\t0.41984682245919913\n",
      "  (4, 302)\t0.38382469349276593\n",
      "  :\t:\n",
      "  (996, 267)\t0.43143487015386384\n",
      "  (996, 206)\t0.228363625108895\n",
      "  (996, 165)\t0.24489932403316297\n",
      "  (996, 94)\t0.297112332161632\n",
      "  (997, 968)\t0.32483123128697605\n",
      "  (997, 861)\t0.39062650687596484\n",
      "  (997, 795)\t0.2135028233839735\n",
      "  (997, 693)\t0.32483123128697605\n",
      "  (997, 653)\t0.39062650687596484\n",
      "  (997, 301)\t0.40785562531402975\n",
      "  (997, 220)\t0.32483123128697605\n",
      "  (997, 63)\t0.40785562531402975\n",
      "  (998, 830)\t0.4389016141357857\n",
      "  (998, 428)\t0.5127503720922321\n",
      "  (998, 391)\t0.474289204940446\n",
      "  (998, 385)\t0.35618471642427474\n",
      "  (998, 133)\t0.4389016141357857\n",
      "  (999, 963)\t0.30043168945763854\n",
      "  (999, 485)\t0.3290206550851362\n",
      "  (999, 374)\t0.4362548871271673\n",
      "  (999, 193)\t0.4035316107575826\n",
      "  (999, 171)\t0.11499160191552907\n",
      "  (999, 20)\t0.29317190115147945\n",
      "  (999, 15)\t0.41782609357133144\n",
      "  (999, 12)\t0.41782609357133144\n",
      "Split the data into training and testing sets:\n",
      "Print the shape of the training set:\n",
      "(800, 1000)\n",
      "Print the shape of the testing set:\n",
      "(200, 1000)\n",
      "Print the shape of the training labels:\n",
      "(800,)\n",
      "Print the shape of the testing labels:\n",
      "(200,)\n",
      "Train the model:\n",
      "Predict the labels:\n",
      "Print the predicted labels:\n",
      "[0 1 1 0 1 0 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1\n",
      " 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0\n",
      " 0 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 1 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1\n",
      " 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0\n",
      " 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0\n",
      " 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1]\n",
      "Evaluate the model:\n",
      "Accuracy:\n",
      "0.955\n",
      "Precision, Recall, F1-Score:\n",
      "(0.9535024154589372, 0.9561190403887033, 0.9545901763414819, None)\n",
      "Confusion Matrix:\n",
      "[[105   6]\n",
      " [  3  86]]\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96       111\n",
      "           1       0.93      0.97      0.95        89\n",
      "\n",
      "    accuracy                           0.95       200\n",
      "   macro avg       0.95      0.96      0.95       200\n",
      "weighted avg       0.96      0.95      0.96       200\n",
      "\n",
      "mean_squared_error:\n",
      "0.045\n",
      "mean_absolute_error:\n",
      "0.045\n",
      "r2_score:\n",
      "0.8177953234133009\n",
      "explained_variance_score:\n",
      "0.8187063467962344\n",
      "Save classification report to csv file:\n",
      "Save confusion matrix to csv file:\n",
      " ------------------------------------ \n",
      " ------------------------------------ \n",
      "Random Forest Classifier:\n",
      "==================================================\n",
      "Take a small sample of the data:\n",
      "TF-IDF Text Vectorizer:\n",
      "Print the shape of the TF-IDF vectorizer:\n",
      "(1000, 1000)\n",
      "Print the type of the TF-IDF vectorizer:\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "Print the TF-IDF vectorizer:\n",
      "  (0, 930)\t0.5223048050592047\n",
      "  (0, 712)\t0.5223048050592047\n",
      "  (0, 594)\t0.2784427624281276\n",
      "  (0, 296)\t0.5890533744095889\n",
      "  (0, 171)\t0.17286159600154732\n",
      "  (1, 952)\t0.5508649598980113\n",
      "  (1, 635)\t0.45807987949302104\n",
      "  (1, 619)\t0.403133452753087\n",
      "  (1, 361)\t0.3279790983002038\n",
      "  (1, 277)\t0.4654285662803492\n",
      "  (2, 871)\t0.5123141812750858\n",
      "  (2, 651)\t0.5123141812750858\n",
      "  (2, 540)\t0.42030256009898087\n",
      "  (2, 181)\t0.5462729329768405\n",
      "  (3, 969)\t0.5636057620646413\n",
      "  (3, 890)\t0.4162984905672204\n",
      "  (3, 608)\t0.2641315453774545\n",
      "  (3, 213)\t0.503710918727204\n",
      "  (3, 171)\t0.1551122115774975\n",
      "  (3, 9)\t0.4018633482043871\n",
      "  (4, 875)\t0.23653488145235566\n",
      "  (4, 608)\t0.19675950372069342\n",
      "  (4, 585)\t0.31954999812067003\n",
      "  (4, 543)\t0.41984682245919913\n",
      "  (4, 302)\t0.38382469349276593\n",
      "  :\t:\n",
      "  (996, 267)\t0.43143487015386384\n",
      "  (996, 206)\t0.228363625108895\n",
      "  (996, 165)\t0.24489932403316297\n",
      "  (996, 94)\t0.297112332161632\n",
      "  (997, 968)\t0.32483123128697605\n",
      "  (997, 861)\t0.39062650687596484\n",
      "  (997, 795)\t0.2135028233839735\n",
      "  (997, 693)\t0.32483123128697605\n",
      "  (997, 653)\t0.39062650687596484\n",
      "  (997, 301)\t0.40785562531402975\n",
      "  (997, 220)\t0.32483123128697605\n",
      "  (997, 63)\t0.40785562531402975\n",
      "  (998, 830)\t0.4389016141357857\n",
      "  (998, 428)\t0.5127503720922321\n",
      "  (998, 391)\t0.474289204940446\n",
      "  (998, 385)\t0.35618471642427474\n",
      "  (998, 133)\t0.4389016141357857\n",
      "  (999, 963)\t0.30043168945763854\n",
      "  (999, 485)\t0.3290206550851362\n",
      "  (999, 374)\t0.4362548871271673\n",
      "  (999, 193)\t0.4035316107575826\n",
      "  (999, 171)\t0.11499160191552907\n",
      "  (999, 20)\t0.29317190115147945\n",
      "  (999, 15)\t0.41782609357133144\n",
      "  (999, 12)\t0.41782609357133144\n",
      "Split the data into training and testing sets:\n",
      "Print the shape of the training set:\n",
      "(800, 1000)\n",
      "Print the shape of the testing set:\n",
      "(200, 1000)\n",
      "Print the shape of the training labels:\n",
      "(800,)\n",
      "Print the shape of the testing labels:\n",
      "(200,)\n",
      "Train the model:\n",
      "Predict the labels:\n",
      "Print the predicted labels:\n",
      "[0 1 1 0 1 0 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1\n",
      " 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0\n",
      " 0 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 1 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1\n",
      " 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0\n",
      " 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0\n",
      " 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1]\n",
      "Evaluate the model:\n",
      "Accuracy:\n",
      "0.96\n",
      "Precision, Recall, F1-Score:\n",
      "(0.9583961410913475, 0.9617370179167932, 0.959673354168767, None)\n",
      "Confusion Matrix:\n",
      "[[105   6]\n",
      " [  2  87]]\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96       111\n",
      "           1       0.94      0.98      0.96        89\n",
      "\n",
      "    accuracy                           0.96       200\n",
      "   macro avg       0.96      0.96      0.96       200\n",
      "weighted avg       0.96      0.96      0.96       200\n",
      "\n",
      "mean_squared_error:\n",
      "0.04\n",
      "mean_absolute_error:\n",
      "0.04\n",
      "r2_score:\n",
      "0.8380402874784897\n",
      "explained_variance_score:\n",
      "0.8396598846037048\n",
      "Save classification report to csv file:\n",
      "Save confusion matrix to csv file:\n",
      " ------------------------------------ \n",
      " ------------------------------------ \n",
      " ------------------------------------ \n",
      " ------------------------------------ \n",
      "Load cleaned dataset:\n",
      "Print the first 5 rows of the dataframe:\n",
      "                                               title  target\n",
      "0  clinical feature cultureproven mycoplasma pneu...       0\n",
      "1  nitric oxide proinflammatory mediator lung dis...       0\n",
      "2         surfactant proteind pulmonary host defense       0\n",
      "3                       role endothelin lung disease       0\n",
      "4  gene expression epithelial cell response pneum...       0\n",
      "Add a new column to the dataframe:\n",
      "Print the first 5 rows of the dataframe:\n",
      "                                               title  target  sentiment\n",
      "0  clinical feature cultureproven mycoplasma pneu...       0      0.000\n",
      "1  nitric oxide proinflammatory mediator lung dis...       0      0.000\n",
      "2         surfactant proteind pulmonary host defense       0      0.128\n",
      "3                       role endothelin lung disease       0      0.000\n",
      "4  gene expression epithelial cell response pneum...       0      0.000\n",
      "Save the dataframe to csv file:\n",
      " ------------------------------------ \n",
      " ------------------------------------ \n",
      "Load sentiments dataset:\n",
      "Sentiment Analysis:\n",
      "==================================================\n",
      "Take a small sample of the data:\n",
      "TF Text Vectorizer:\n",
      "Including the sentiment scores:\n",
      "Prepare data for sentiment analysis:\n",
      "Train the model:\n",
      "Predict the labels:\n",
      "Print the predicted labels:\n",
      "[0 1 1 0 1 0 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1\n",
      " 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0\n",
      " 0 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 1 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1\n",
      " 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0\n",
      " 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0\n",
      " 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1]\n",
      "Evaluate the model:\n",
      "Accuracy:\n",
      "0.955\n",
      "Precision, Recall, F1-Score:\n",
      "(0.9535024154589372, 0.9561190403887033, 0.9545901763414819, None)\n",
      "Confusion Matrix:\n",
      "[[105   6]\n",
      " [  3  86]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96       111\n",
      "           1       0.93      0.97      0.95        89\n",
      "\n",
      "    accuracy                           0.95       200\n",
      "   macro avg       0.95      0.96      0.95       200\n",
      "weighted avg       0.96      0.95      0.96       200\n",
      "\n",
      "Mean Squared Error:\n",
      "0.045\n",
      "Mean Absolute Error:\n",
      "0.045\n",
      "R2 Score:\n",
      "0.8177953234133009\n",
      "Explained Variance Score:\n",
      "0.8187063467962344\n",
      "Save classification report to csv file:\n",
      "Save confusion matrix to csv file:\n",
      " ------------------------------------ \n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\nREAD ME\\nmetadata.csv is the dataset that is used for this project.\\nThe dataset is taken from the following link:\\nhttps://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge\\n\\nThe dataset is a collection of research papers related to COVID-19, SARS-CoV-2, and related coronaviruses.\\n\\nThe dataset contains the following columns:\\ncord_uid, sha, source_x, title, doi, pmcid, pubmed_id, license, abstract, publish_time, authors, journal,\\nmag_id, who_covidence_id, arxiv_id, pdf_json_files, pmc_json_files, url, s2_id\\n\\n\\n'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PROGRAM HEADER NAME: KEVIN MASTASCUSA DESCRIPTION: This program is a data science project that uses the CORD-19\n",
    "# dataset to perform sentiment analysis on the abstracts of the papers. The program will use the abstracts to\n",
    "# determine if the paper is about COVID-19 or not. DATE: VERSION: 1.0 USAGE: python main.py\n",
    "# ================================================================================================================\n",
    "\n",
    "import string  # To remove any punctuations\n",
    "import matplotlib.pyplot as plt  # For data visualization\n",
    "import numpy as np\n",
    "# Import the required libraries\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import seaborn as sns  # For data visualization\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer  # To perform lemmatization\n",
    "from nltk.stem.porter import PorterStemmer  # To perform stemming\n",
    "from nltk.tokenize import word_tokenize  # To create tokens from text\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, \\\n",
    "    classification_report, precision_recall_fscore_support, \\\n",
    "    mean_squared_error, mean_absolute_error, r2_score, \\\n",
    "    explained_variance_score  # To measure how well the model is performing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB  # Naive Bayes classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier  # KNN classifier\n",
    "# import decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# import random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# nltk.download('punkt')  # punkt is a pre-trained model that helps you tokenize words and sentences\n",
    "# nltk.download('wordnet')  # wordnet is a lexical database for the English language\n",
    "# nltk.download('stopwords')  # stopwords are the words in any language which does not add much meaning to a sentence\n",
    "\n",
    "\n",
    "# warnings.filterwarnings('ignore')\n",
    "# To ignore any warnings\n",
    "# stops = set(stopwords.words('english'))\n",
    "# To remove the stopwords\n",
    "# print(stops)\n",
    "# To print the stopwords\n",
    "\n",
    "\n",
    "# nltk.download()\n",
    "\n",
    "\n",
    "def print_hi(name):\n",
    "    \"\"\"\n",
    "    This function prints the name of the user\n",
    "\n",
    "\n",
    "\n",
    "    :param name:\n",
    "    \"\"\"\n",
    "    print(f'Hi , {name}')\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(f'Welcome to the CORD-19 Sentiment Analysis Project')\n",
    "    print(f'Please select an option from the menu below')\n",
    "    print(f'1. Perform Sentiment Analysis on the CORD-19 Dataset')\n",
    "    print(f'2. Exit')\n",
    "    choice = int(input(f'Enter your choice: '))\n",
    "    if choice == 1:\n",
    "        print(f'Performing Sentiment Analysis on the CORD-19 Dataset')\n",
    "        print(f'Please wait while the program loads the dataset')\n",
    "    if choice == 2:\n",
    "        print(f'Exiting the program')\n",
    "        exit()\n",
    "    else:\n",
    "        print(f'Invalid choice. Please try again')\n",
    "        main()\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    This function performs the following operations:\n",
    "    1. Removes the punctuations\n",
    "    2. Removes the stopwords\n",
    "    3. Performs stemming\n",
    "    4. Performs lemmatization\n",
    "\n",
    "    :param text:\n",
    "    :return:\n",
    "\n",
    "    Example:\n",
    "    Input: \"This is a sample text!!!\"\n",
    "\n",
    "    Output: \"sample text\"\n",
    "    \"\"\"\n",
    "    # Remove the punctuations\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Convert the text to lowercase\n",
    "    text = text.lower()\n",
    "    # Create tokens from the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove the stopwords\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "    # Perform stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    # Perform lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # Join the tokens to form the text\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Function to plot the confusion matrix\n",
    "def plot_confusion_matrix(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Function to plot the confusion matrix\n",
    "\n",
    "\n",
    "    :param y_test:\n",
    "    :param y_pred:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='g')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('Actual Labels')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# KNN\n",
    "# Function to train the KNN model\n",
    "def train_knn(X_train, y_train, k):\n",
    "    \"\"\"\n",
    "    Function to train the KNN model\n",
    "\n",
    "    :param X_train:\n",
    "    :param y_train:\n",
    "    :param k:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    return knn\n",
    "\n",
    "\n",
    "# Function to test the KNN model\n",
    "def test_knn(knn, X_test):\n",
    "    \"\"\"\n",
    "    Function to test the KNN model\n",
    "\n",
    "    :param knn:\n",
    "    :param X_test:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    y_pred = knn.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "# Function to perform KNN classification\n",
    "def knn_classification(X_train, y_train, X_test, y_test, k):\n",
    "    \"\"\"\n",
    "    Function to perform KNN classification\n",
    "\n",
    "    :param X_train:\n",
    "\n",
    "    :param y_train:\n",
    "    :param X_test:\n",
    "    :param y_test:\n",
    "    :param k:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    knn = train_knn(X_train, y_train, k)\n",
    "    y_pred = test_knn(knn, X_test)\n",
    "    print('Accuracy Score: ', accuracy_score(y_test, y_pred))\n",
    "    print('Confusion Matrix: \\n', confusion_matrix(y_test, y_pred))\n",
    "    print('Classification Report: \\n', classification_report(y_test, y_pred))\n",
    "    plot_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "# Naive Bayes\n",
    "# Function to train the Naive Bayes model\n",
    "def train_naive_bayes(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Function to train the Naive Bayes model\n",
    "  :param X_train:\n",
    "    :param y_train:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train, y_train)\n",
    "    return nb\n",
    "\n",
    "\n",
    "# Function to test the Naive Bayes model\n",
    "def test_naive_bayes(nb, X_test):\n",
    "    \"\"\"\n",
    "    Function to test the Naive Bayes model\n",
    "\n",
    "    :param nb:\n",
    "    :param X_test:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    y_pred = nb.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "# Create Class Attribute For Target Variable\n",
    "def create_class_attribute(df):\n",
    "    \"\"\"\n",
    "    Create Class Attribute For Target Variable\n",
    "\n",
    "\n",
    "    :param df:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    df['class'] = df['Sentiment'].apply(lambda x: 1 if x == 'covid-19' else 0)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Create Sentiment Description Attribute For Target Variable\n",
    "def create_sentiment_description_attribute(df):\n",
    "    \"\"\"\n",
    "    Create Sentiment Description Attribute For Target Variable\n",
    "\n",
    "    :param df:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    df['Sentiment_Description'] = df['Sentiment'].apply(lambda x: 'Positive' if x == 'covid-19' else 'Negative')\n",
    "    return df\n",
    "\n",
    "\n",
    "# CREATE DOMAIN ATTRIBUTE FOR TARGET VARIABLE\n",
    "def create_domain_attribute(df):\n",
    "    \"\"\"\n",
    "    CREATE DOMAIN ATTRIBUTE FOR TARGET VARIABLE\n",
    "\n",
    "    :param df:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    df['Domain'] = df['Sentiment'].apply(lambda x: 'COVID-19' if x == 'covid-19' else 'Other')\n",
    "    return df\n",
    "\n",
    "\n",
    "# Function to Preprocess the data\n",
    "def preprocess_data(df):\n",
    "    # Remove the rows with missing values\n",
    "    df.dropna(inplace=True)\n",
    "    # Remove the duplicates\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    # Remove the rows with the class label as 'other'\n",
    "    df = df[df['Sentiment'] != 'other']\n",
    "    # Clean the text\n",
    "    df['cleaned_text'] = df['Abstract'].apply(clean_text)\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print_hi('Kevin Mastascusa')\n",
    "    # Load the data\n",
    "    df = pd.read_csv('metadata.csv', low_memory=False)\n",
    "    # Print the structure of the data\n",
    "    print(df.info())\n",
    "    # Print the first 10 rows of the data\n",
    "    print(df.head(10))\n",
    "    # Print a summary of the data\n",
    "    print(df.describe(include='all'))\n",
    "    # Print the number of missing values in each column\n",
    "    print(df.isnull().sum())\n",
    "    # Print the number of unique values in each column\n",
    "    print(df.nunique())\n",
    "\n",
    "print('=' * 500)\n",
    "print('Data Preprocessing')\n",
    "print('=' * 500)\n",
    "# Preprocess the data\n",
    "print('Drop the columns that are not required:')\n",
    "df.drop(['cord_uid', 'sha', 'source_x', 'doi', 'pmcid', 'pubmed_id', 'license', 'abstract', 'publish_time', 'authors',\n",
    "         'journal', 'mag_id', 'who_covidence_id', 'arxiv_id', 'pdf_json_files', 'pmc_json_files', 'url', 's2_id'],\n",
    "        axis=1, inplace=True)\n",
    "# Check the structure of the data\n",
    "print('Check the structure of the data:')\n",
    "print(df.info())\n",
    "# Print the first 10 rows of the data\n",
    "print('Print the first 10 rows of the data:')\n",
    "print(df.head(10))\n",
    "# Print a summary of the data\n",
    "print('Print a summary of the data:')\n",
    "print(df.describe(include='all'))\n",
    "# Print the number of missing values in each column\n",
    "print('Number of missing values in each column:')\n",
    "print(df.isnull().sum())\n",
    "# Print the number of unique values in each column\n",
    "print('Number of unique values in each column:')\n",
    "print(df.nunique())\n",
    "# Drop the rows with missing values\n",
    "print('Drop the rows with missing values:')\n",
    "df.dropna(inplace=True)\n",
    "# Print the number of missing values in each column\n",
    "print('Number of missing values in each column:')\n",
    "print(df.isnull().sum())\n",
    "# Print the number of unique values in each column\n",
    "print('Number of unique values in each column:')\n",
    "print(df.nunique())\n",
    "\n",
    "print(\"MARKER 1\")\n",
    "print(' ------------------------------------ ')\n",
    "print(' ------------------------------------ ')\n",
    "# Add target column for classification.\n",
    "print('Add target column for classification:')\n",
    "df['target'] = df['title'].apply(lambda x: 1 if 'covid-19' in x.lower() else 0)\n",
    "# add target\n",
    "# Print the first 10 rows of the data\n",
    "print('Print the first 10 rows of the data:')\n",
    "print(df.head(10))\n",
    "# Print the number of missing values in each column\n",
    "print('Number of missing values in each column:')\n",
    "print(df.isnull().sum())\n",
    "# Print the number of unique values in each column\n",
    "print('Number of unique values in each column:')\n",
    "print(df.nunique())\n",
    "# Print the class distribution\n",
    "print('Class distribution:')\n",
    "print(df['target'].value_counts())\n",
    "# Plot the class distribution\n",
    "print('Class distribution plot:')\n",
    "sns.countplot(df['target'])\n",
    "plt.show()\n",
    "print(' ------------------------------------ ')\n",
    "print(' ------------------------------------ ')\n",
    "\n",
    "print(\"MARKER 2\")\n",
    "print(' ------------------------------------ ')\n",
    "print(' ------------------------------------ ')\n",
    "\n",
    "print('Data Preprocessing:')\n",
    "print('NUMPY ARRAY CONVERSION:')\n",
    "print('Convert the dataframe to numpy array:')\n",
    "numpy_array = df.to_numpy()\n",
    "print('Print the numpy array:')\n",
    "print(numpy_array)\n",
    "print('Print the shape of the numpy array:')\n",
    "print(numpy_array.shape)\n",
    "print('Print the type of the numpy array:')\n",
    "print(type(numpy_array))\n",
    "\n",
    "print('Print the first row of the numpy array:')\n",
    "print(numpy_array[0])\n",
    "print('Print the second row of the numpy array:')\n",
    "print(numpy_array[1])\n",
    "print('Print the third row of the numpy array:')\n",
    "print(numpy_array[2])\n",
    "print('Print the fourth row of the numpy array:')\n",
    "print(numpy_array[3])\n",
    "print('Print the fifth row of the numpy array:')\n",
    "print(numpy_array[4])\n",
    "\n",
    "print('Print the first column of the numpy array:')\n",
    "print(numpy_array[:, 0])\n",
    "print('Print the second column of the numpy array:')\n",
    "print(numpy_array[:, 1])\n",
    "\n",
    "print('Print the first row and first column of the numpy array:')\n",
    "print(numpy_array[0][0])\n",
    "\n",
    "print('Print the first row and second column of the numpy array:')\n",
    "print(numpy_array[0][1])\n",
    "\n",
    "# Export as csv file\n",
    "print('Export as csv file:')\n",
    "df.to_csv('CORD19_abstract.csv', index=False)\n",
    "var = open('CORD19_abstract.csv', 'r', encoding='utf-8').readlines()[:5]\n",
    "print(var)\n",
    "\n",
    "print(' ------------------------------------ ')\n",
    "print(' ------------------------------------ ')\n",
    "\n",
    "print('text cleaning:')\n",
    "# Convert to lower case\n",
    "print('Convert to lower case:')\n",
    "df['title'] = df['title'].apply(lambda x: x.lower())\n",
    "# Remove punctuation\n",
    "print('Remove punctuation:')\n",
    "df['title'] = df['title'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "# Remove numbers\n",
    "print('Remove numbers:')\n",
    "df['title'] = df['title'].apply(lambda x: x.translate(str.maketrans('', '', string.digits)))\n",
    "# Remove whitespaces\n",
    "print('Remove whitespaces:')\n",
    "df['title'] = df['title'].apply(lambda x: x.strip())\n",
    "# Remove stopwords\n",
    "print('Remove stopwords:')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "# Remove short words\n",
    "print('Remove short words:')\n",
    "df['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if len(word) > 3]))\n",
    "# Lemmatization\n",
    "print('Lemmatization:')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['title'] = df['title'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "# Print the first 5 rows of the data\n",
    "print('Print the first 5 rows of the data:')\n",
    "print(df.head())\n",
    "# Print the number of missing values in each column\n",
    "\n",
    "print('Number of missing values in each column:')\n",
    "print(df.isnull().sum())\n",
    "# Print the number of unique values in each column\n",
    "print('Number of unique values in each column:')\n",
    "print(df.nunique())\n",
    "\n",
    "save_path = 'CORD19_abstract_cleaned.csv'\n",
    "df.to_csv(save_path, index=False)\n",
    "var = open(save_path, 'r', encoding='utf-8').readlines()[:5]\n",
    "print(var)\n",
    "\n",
    "print(' ------------------------------------ ')\n",
    "\n",
    "# TF\n",
    "print('TF Text Vectorizer:')\n",
    "tf_binary_vectorizer = CountVectorizer(binary=True,\n",
    "                                       max_features=1000)  # max_features=1000 to limit the number of features (vocabulary) to 1000\n",
    "# binary=True to get binary outputs (0 or 1) instead of counts (1, 2, 3, etc.)\n",
    "# Fit the vectorizer\n",
    "tf_binary_vectorizer.fit(df['title'])\n",
    "X = tf_binary_vectorizer.transform(df['title'])\n",
    "print('Print the shape of the TF Binary vectorizer:')\n",
    "print(X.shape)\n",
    "print('Print the type of the TF Binary vectorizer:')\n",
    "print(type(X))\n",
    "print('Print the TF Binary vectorizer:')\n",
    "print(X)\n",
    "print('Print the TF Binary vectorizer to array:')\n",
    "print(X.toarray())\n",
    "print('Print the TF Binary vectorizer to array shape:')\n",
    "print(X.toarray().shape)\n",
    "print('Print the TF Binary vectorizer to array type:')\n",
    "print(type(X.toarray()))\n",
    "print('Print the TF Binary vectorizer to array first row:')\n",
    "print(X.toarray()[0])\n",
    "print('Print the TF Binary vectorizer to array first row shape:')\n",
    "print(X.toarray()[0].shape)\n",
    "print('Print the TF Binary vectorizer to array first row type:')\n",
    "print(type(X.toarray()[0]))\n",
    "\n",
    "# TF-IDF\n",
    "print(\n",
    "    'TF-IDF Text Vectorizer:')  # TF-IDF is a combination of TF and IDF (Inverse Document Frequency) which is a measure of how important a word is in a document\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=1000)  # max_features=1000 to limit the number of features (vocabulary) to 1000\n",
    "# Fit the vectorizer\n",
    "tfidf_vectorizer.fit(\n",
    "    df['title'])  # The vectorizer learns the vocabulary from the data and assigns an index to each word\n",
    "X = tfidf_vectorizer.transform(df['title'])  # The vectorizer contains the TF-IDF values for each word in each document\n",
    "print('Print the shape of the TF-IDF vectorizer:')  # The shape is (number of documents, number of features)\n",
    "print(X.shape)\n",
    "print('Print the type of the TF-IDF vectorizer:')  # The type is sparse matrix\n",
    "print(type(X))\n",
    "print('Print the TF-IDF vectorizer:')  # The vectorizer contains the TF-IDF values for each word in each document\n",
    "print(X)\n",
    "print('Print the TF-IDF vectorizer to array:')  # The array contains the TF-IDF values for each word in each document\n",
    "print(X.toarray())\n",
    "print('Print the TF-IDF vectorizer to array shape:')  # The shape is (number of documents, number of features)\n",
    "print(X.toarray().shape)\n",
    "print('Print the TF-IDF vectorizer to array type:')  # The type is numpy array\n",
    "print(type(X.toarray()))\n",
    "print('Print the TF-IDF vectorizer to array first row:')  # The first row is the first document\n",
    "print(X.toarray()[0])\n",
    "print('Print the TF-IDF vectorizer to array first row shape:')  # The shape is (number of features,)\n",
    "print(X.toarray()[0].shape)\n",
    "\n",
    "print(' ------------------------------------ ')\n",
    "print(' ------------------------------------ ')\n",
    "print(' ------------------------------------ ')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "print('Split the data into training and testing sets:')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df['target'], test_size=0.2, random_state=42)\n",
    "print('Print the shape of the training set:')\n",
    "print(X_train.shape)\n",
    "print('Print the shape of the testing set:')\n",
    "print(X_test.shape)\n",
    "print('Print the shape of the training labels:')\n",
    "print(y_train.shape)\n",
    "print('Print the shape of the testing labels:')\n",
    "print(y_test.shape)\n",
    "\n",
    "# Train the model\n",
    "print('Train the model:')\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels\n",
    "print('Predict the labels:')\n",
    "y_pred_binary = model.predict(X_test)\n",
    "print('Print the predicted labels:')\n",
    "print(y_pred_binary)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Evaluate the model:')\n",
    "print('Accuracy:')\n",
    "print(accuracy_score(y_test,\n",
    "                     y_pred_binary))  # Accuracy is the ratio of correctly predicted observations to the total\n",
    "# observations\n",
    "print(\n",
    "    'Precision, Recall, F1-Score:')  # Precision is the ratio of correctly predicted positive observations to the total predicted positive observations\n",
    "print(precision_recall_fscore_support(y_test, y_pred_binary,\n",
    "                                      average='macro'))  # average='macro' to get the average of the precision, recall, and F1-score of the two classes\n",
    "print('Confusion Matrix:')  #\n",
    "print(confusion_matrix(y_test, y_pred_binary))\n",
    "\n",
    "print(' ------------------------------------ ')\n",
    "\n",
    "# KNN Classifier\n",
    "\n",
    "print('KNN Classifier:')\n",
    "print('=' * 50)\n",
    "# Take a small sample of the data\n",
    "print('Take a small sample of the data:')\n",
    "df_sample = df.sample(n=1000, random_state=42)\n",
    "print('Print the shape of the sample:')\n",
    "print(df_sample.shape)\n",
    "\n",
    "# TF-IDF\n",
    "print(\n",
    "    'TF-IDF Text Vectorizer:')  # TF-IDF is a combination of TF and IDF (Inverse Document Frequency) which is a measure of how important a word is in a document\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=1000)  # max_features=1000 to limit the number of features (vocabulary) to 1000\n",
    "# Fit the vectorizer\n",
    "tfidf_vectorizer.fit(\n",
    "    df_sample['title'])  # The vectorizer learns the vocabulary from the data and assigns an index to each word\n",
    "X = tfidf_vectorizer.transform(\n",
    "    df_sample['title'])  # The vectorizer contains the TF-IDF values for each word in each document\n",
    "print('Print the shape of the TF-IDF vectorizer:')  # The shape is (number of documents, number of features)\n",
    "print(X.shape)\n",
    "print('Print the type of the TF-IDF vectorizer:')  # The type is sparse matrix\n",
    "print(type(X))\n",
    "print('Print the TF-IDF vectorizer:')  # The vectorizer contains the TF-IDF values for each word in each document\n",
    "print(X)\n",
    "print('Print the TF-IDF vectorizer to array:')  # The array contains the TF-IDF values for each word in each document\n",
    "print(X.toarray())\n",
    "print('Print the TF-IDF vectorizer to array shape:')  # The shape is (number of documents, number of features)\n",
    "print(X.toarray().shape)\n",
    "print('Print the TF-IDF vectorizer to array type:')  # The type is numpy array\n",
    "print(type(X.toarray()))\n",
    "print('Print the TF-IDF vectorizer to array first row:')  # The first row is the first document\n",
    "print(X.toarray()[0])\n",
    "print('Print the TF-IDF vectorizer to array first row shape:')  # The shape is (number of features,)\n",
    "print(X.toarray()[0].shape)\n",
    "\n",
    "print(' ------------------------------------ ')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "print('Split the data into training and testing sets:')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df_sample['target'], test_size=0.2, random_state=42)\n",
    "print('Print the shape of the training set:')\n",
    "print(X_train.shape)\n",
    "print('Print the shape of the testing set:')\n",
    "print(X_test.shape)\n",
    "print('Print the shape of the training labels:')\n",
    "print(y_train.shape)\n",
    "print('Print the shape of the testing labels:')\n",
    "print(y_test.shape)\n",
    "\n",
    "# Train the model\n",
    "print('Train the model:')\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels\n",
    "print('Predict the labels:')\n",
    "y_pred_binary = model.predict(X_test)\n",
    "print('Print the predicted labels:')\n",
    "print(y_pred_binary)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Evaluate the model:')\n",
    "print('Accuracy:')\n",
    "print(accuracy_score(y_test,\n",
    "                     y_pred_binary))  # Accuracy is the ratio of correctly predicted observations to the total observations\n",
    "print(\n",
    "    'Precision, Recall, F1-Score:')  # Precision is the ratio of correctly predicted positive observations to the total predicted positive observations\n",
    "print(precision_recall_fscore_support(y_test, y_pred_binary,\n",
    "                                      average='macro'))  # average='macro' to get the average of the precision, recall, and F1-score of the two classes\n",
    "print('Confusion Matrix:')  #\n",
    "print(confusion_matrix(y_test, y_pred_binary))\n",
    "print('classification_report:')\n",
    "print(classification_report(y_test, y_pred_binary))\n",
    "print('mean_squared_error:')\n",
    "print(mean_squared_error(y_test, y_pred_binary))\n",
    "print('mean_absolute_error:')\n",
    "print(mean_absolute_error(y_test, y_pred_binary))\n",
    "print('r2_score:')\n",
    "print(r2_score(y_test, y_pred_binary))\n",
    "print('explained_variance_score:')\n",
    "print(explained_variance_score(y_test, y_pred_binary))\n",
    "\n",
    "# Save classification report to csv file\n",
    "print('Save classification report to csv file:')\n",
    "report = classification_report(y_test, y_pred_binary, output_dict=True)\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "df_report.to_csv('classification_reportKNNClassifier5N.csv', index=False)\n",
    "\n",
    "# Save confusion matrix to csv file\n",
    "print('Save confusion matrix to csv file:')\n",
    "df_confusion_matrix = pd.DataFrame(confusion_matrix(y_test, y_pred_binary))\n",
    "df_confusion_matrix.to_csv('confusion_matrixKNNClassifier5N.csv', index=False)\n",
    "\n",
    "print(' ------------------------------------ ')\n",
    "\n",
    "# Naive Bayes Classifier\n",
    "\n",
    "print('Naive Bayes Classifier:')\n",
    "print('=' * 50)\n",
    "# Take a small sample of the data\n",
    "print('Take a small sample of the data:')\n",
    "df_sample = df.sample(n=1000, random_state=42)\n",
    "print('Print the shape of the sample:')\n",
    "print(df_sample.shape)\n",
    "\n",
    "# TF-IDF\n",
    "print(\n",
    "    'TF-IDF Text Vectorizer:')  # TF-IDF is a combination of TF and IDF (Inverse Document Frequency) which is a measure of how important a word is in a document\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=1000)  # max_features=1000 to limit the number of features (vocabulary) to 1000\n",
    "# Fit the vectorizer\n",
    "tfidf_vectorizer.fit(\n",
    "    df_sample['title'])  # The vectorizer learns the vocabulary from the data and assigns an index to each word\n",
    "X = tfidf_vectorizer.transform(\n",
    "    df_sample['title'])  # The vectorizer contains the TF-IDF values for each word in each document\n",
    "print('Print the shape of the TF-IDF vectorizer:')  # The shape is (number of documents, number of features)\n",
    "print(X.shape)\n",
    "print('Print the type of the TF-IDF vectorizer:')  # The type is sparse matrix\n",
    "print(type(X))\n",
    "print('Print the TF-IDF vectorizer:')  # The vectorizer contains the TF-IDF values for each word in each document\n",
    "print(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "print('Split the data into training and testing sets:')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df_sample['target'], test_size=0.2, random_state=42)\n",
    "print('Print the shape of the training set:')\n",
    "print(X_train.shape)\n",
    "print('Print the shape of the testing set:')\n",
    "print(X_test.shape)\n",
    "print('Print the shape of the training labels:')\n",
    "print(y_train.shape)\n",
    "print('Print the shape of the testing labels:')\n",
    "print(y_test.shape)\n",
    "\n",
    "# Train the model\n",
    "print('Train the model:')\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels\n",
    "print('Predict the labels:')\n",
    "y_pred_binary = model.predict(X_test)\n",
    "print('Print the predicted labels:')\n",
    "print(y_pred_binary)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Evaluate the model:')\n",
    "print('Accuracy:')\n",
    "print(accuracy_score(y_test,\n",
    "                     y_pred_binary))  # Accuracy is the ratio of correctly predicted observations to the total observations\n",
    "\n",
    "print(\n",
    "    'Precision, Recall, F1-Score:')  # Precision is the ratio of correctly predicted positive observations to the total predicted positive observations\n",
    "print(precision_recall_fscore_support(y_test, y_pred_binary,\n",
    "                                      average='macro'))  # average='macro' to get the average of the precision, recall, and F1-score of the two classes\n",
    "\n",
    "print('Confusion Matrix:')  #\n",
    "print(confusion_matrix(y_test, y_pred_binary))\n",
    "print('classification_report:')\n",
    "\n",
    "print(classification_report(y_test, y_pred_binary))\n",
    "print('mean_squared_error:')\n",
    "print(mean_squared_error(y_test, y_pred_binary))\n",
    "print('mean_absolute_error:')\n",
    "print(mean_absolute_error(y_test, y_pred_binary))\n",
    "print('r2_score:')\n",
    "print(r2_score(y_test, y_pred_binary))\n",
    "print('explained_variance_score:')\n",
    "print(explained_variance_score(y_test, y_pred_binary))\n",
    "print(' ------------------------------------ ')\n",
    "\n",
    "# Save classification report to csv file\n",
    "print('Save classification report to csv file:')\n",
    "report = classification_report(y_test, y_pred_binary, output_dict=True)\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "df_report.to_csv('classification_reportNaiveBayesClassifier.csv', index=False)\n",
    "\n",
    "# Save confusion matrix to csv file\n",
    "print('Save confusion matrix to csv file:')\n",
    "df_confusion_matrix = pd.DataFrame(confusion_matrix(y_test, y_pred_binary))\n",
    "df_confusion_matrix.to_csv('confusion_matrixNaiveBayesClassifier.csv', index=False)\n",
    "\n",
    "# Decision Tree Classifier\n",
    "\n",
    "print('Decision Tree Classifier:')\n",
    "print('=' * 50)\n",
    "# Take a small sample of the data\n",
    "print('Take a small sample of the data:')\n",
    "df_sample = df.sample(n=1000, random_state=42)\n",
    "\n",
    "# TF-IDF\n",
    "print(\n",
    "    'TF-IDF Text Vectorizer:')  # TF-IDF is a combination of TF and IDF (Inverse Document Frequency) which is a measure of how important a word is in a document\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=1000)  # max_features=1000 to limit the number of features (vocabulary) to 1000\n",
    "# Fit the vectorizer\n",
    "tfidf_vectorizer.fit(\n",
    "    df_sample['title'])  # The vectorizer learns the vocabulary from the data and assigns an index to each word\n",
    "X = tfidf_vectorizer.transform(\n",
    "    df_sample['title'])  # The vectorizer contains the TF-IDF values for each word in each document\n",
    "print('Print the shape of the TF-IDF vectorizer:')  # The shape is (number of documents, number of features)\n",
    "print(X.shape)\n",
    "print('Print the type of the TF-IDF vectorizer:')  # The type is sparse matrix\n",
    "print(type(X))\n",
    "print('Print the TF-IDF vectorizer:')  # The vectorizer contains the TF-IDF values for each word in each document\n",
    "print(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "print('Split the data into training and testing sets:')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df_sample['target'], test_size=0.2, random_state=42)\n",
    "print('Print the shape of the training set:')\n",
    "print(X_train.shape)\n",
    "\n",
    "print('Print the shape of the testing set:')\n",
    "print(X_test.shape)\n",
    "\n",
    "print('Print the shape of the training labels:')\n",
    "print(y_train.shape)\n",
    "\n",
    "print('Print the shape of the testing labels:')\n",
    "print(y_test.shape)\n",
    "\n",
    "# Train the model\n",
    "print('Train the model:')\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels\n",
    "print('Predict the labels:')\n",
    "\n",
    "y_pred_binary = model.predict(X_test)\n",
    "print('Print the predicted labels:')\n",
    "print(y_pred_binary)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Evaluate the model:')\n",
    "print('Accuracy:')\n",
    "print(accuracy_score(y_test,\n",
    "                     y_pred_binary))  # Accuracy is the ratio of correctly predicted observations to the total observations\n",
    "\n",
    "print(\n",
    "    'Precision, Recall, F1-Score:')  # Precision is the ratio of correctly predicted positive observations to the total predicted positive observations\n",
    "print(precision_recall_fscore_support(y_test, y_pred_binary,\n",
    "                                      average='macro'))  # average='macro' to get the average of the precision, recall, and F1-score of the two classes\n",
    "\n",
    "print('Confusion Matrix:')  #\n",
    "print(confusion_matrix(y_test, y_pred_binary))\n",
    "print('classification_report:')\n",
    "print(classification_report(y_test, y_pred_binary))\n",
    "print('mean_squared_error:')\n",
    "print(mean_squared_error(y_test, y_pred_binary))\n",
    "print('mean_absolute_error:')\n",
    "print(mean_absolute_error(y_test, y_pred_binary))\n",
    "print('r2_score:')\n",
    "print(r2_score(y_test, y_pred_binary))\n",
    "print('explained_variance_score:')\n",
    "print(explained_variance_score(y_test, y_pred_binary))\n",
    "\n",
    "# Save classification report to csv file\n",
    "print('Save classification report to csv file:')\n",
    "report = classification_report(y_test, y_pred_binary, output_dict=True)\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "df_report.to_csv('classification_reportDecisionTreeClassifier.csv', index=False)\n",
    "\n",
    "# Save confusion matrix to csv file\n",
    "print('Save confusion matrix to csv file:')\n",
    "df_confusion_matrix = pd.DataFrame(confusion_matrix(y_test, y_pred_binary))\n",
    "df_confusion_matrix.to_csv('confusion_matrixDecisionTreeClassifier.csv', index=False)\n",
    "print(' ------------------------------------ ')\n",
    "\n",
    "print(' ------------------------------------ ')\n",
    "\n",
    "# Random Forest Classifier\n",
    "\n",
    "print('Random Forest Classifier:')\n",
    "print('=' * 50)\n",
    "# Take a small sample of the data\n",
    "print('Take a small sample of the data:')\n",
    "df_sample = df.sample(n=1000, random_state=42)\n",
    "\n",
    "# TF-IDF\n",
    "print(\n",
    "    'TF-IDF Text Vectorizer:')  # TF-IDF is a combination of TF and IDF (Inverse Document Frequency) which is a measure of how important a word is in a document\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=1000)  # max_features=1000 to limit the number of features (vocabulary) to 1000\n",
    "# Fit the vectorizer\n",
    "tfidf_vectorizer.fit(\n",
    "    df_sample['title'])  # The vectorizer learns the vocabulary from the data and assigns an index to each word\n",
    "X = tfidf_vectorizer.transform(\n",
    "    df_sample['title'])  # The vectorizer contains the TF-IDF values for each word in each document\n",
    "print('Print the shape of the TF-IDF vectorizer:')  # The shape is (number of documents, number of features)\n",
    "print(X.shape)\n",
    "print('Print the type of the TF-IDF vectorizer:')  # The type is sparse matrix\n",
    "print(type(X))\n",
    "print('Print the TF-IDF vectorizer:')  # The vectorizer contains the TF-IDF values for each word in each document\n",
    "print(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "print('Split the data into training and testing sets:')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df_sample['target'], test_size=0.2, random_state=42)\n",
    "print('Print the shape of the training set:')\n",
    "print(X_train.shape)\n",
    "\n",
    "print('Print the shape of the testing set:')\n",
    "print(X_test.shape)\n",
    "\n",
    "print('Print the shape of the training labels:')\n",
    "print(y_train.shape)\n",
    "\n",
    "print('Print the shape of the testing labels:')\n",
    "print(y_test.shape)\n",
    "\n",
    "# Train the model\n",
    "print('Train the model:')\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels\n",
    "print('Predict the labels:')\n",
    "y_pred_binary = model.predict(X_test)\n",
    "print('Print the predicted labels:')\n",
    "print(y_pred_binary)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Evaluate the model:')\n",
    "\n",
    "print('Accuracy:')\n",
    "print(accuracy_score(y_test,\n",
    "                     y_pred_binary))  # Accuracy is the ratio of correctly predicted observations to the total observations\n",
    "\n",
    "print(\n",
    "    'Precision, Recall, F1-Score:')  # Precision is the ratio of correctly predicted positive observations to the total predicted positive observations\n",
    "print(precision_recall_fscore_support(y_test, y_pred_binary,\n",
    "                                      average='macro'))  # average='macro' to get the average of the precision, recall, and F1-score of the two classes\n",
    "\n",
    "print('Confusion Matrix:')  #\n",
    "print(confusion_matrix(y_test, y_pred_binary))\n",
    "print('classification_report:')\n",
    "print(classification_report(y_test, y_pred_binary))\n",
    "print('mean_squared_error:')\n",
    "print(mean_squared_error(y_test, y_pred_binary))\n",
    "print('mean_absolute_error:')\n",
    "print(mean_absolute_error(y_test, y_pred_binary))\n",
    "print('r2_score:')\n",
    "print(r2_score(y_test, y_pred_binary))\n",
    "print('explained_variance_score:')\n",
    "print(explained_variance_score(y_test, y_pred_binary))\n",
    "\n",
    "# Save classification report to csv file\n",
    "print('Save classification report to csv file:')\n",
    "report = classification_report(y_test, y_pred_binary, output_dict=True)\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "df_report.to_csv('classification_report.csv', index=False)\n",
    "\n",
    "# Save confusion matrix to csv file\n",
    "print('Save confusion matrix to csv file:')\n",
    "df_confusion_matrix = pd.DataFrame(confusion_matrix(y_test, y_pred_binary))\n",
    "df_confusion_matrix.to_csv('confusion_matrix.csv', index=False)\n",
    "\n",
    "print(' ------------------------------------ ')\n",
    "\n",
    "print(' ------------------------------------ ')\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "def get_sentiment(text):\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    return sentiment['compound']  # return the compound score\n",
    "\n",
    "\n",
    "print(' ------------------------------------ ')\n",
    "\n",
    "print(' ------------------------------------ ')\n",
    "\n",
    "# Load cleaned dataset\n",
    "print('Load cleaned dataset:')\n",
    "df = pd.read_csv('CORD19_abstract_cleaned.csv')\n",
    "\n",
    "# print the first 5 rows of the dataframe\n",
    "print('Print the first 5 rows of the dataframe:')\n",
    "print(df.head())\n",
    "\n",
    "# add a new column to the dataframe\n",
    "print('Add a new column to the dataframe:')\n",
    "df['title'] = df['title'].fillna('')\n",
    "df['sentiment'] = df['title'].apply(get_sentiment)\n",
    "\n",
    "# print the first 5 rows of the dataframe\n",
    "print('Print the first 5 rows of the dataframe:')\n",
    "print(df.head())\n",
    "\n",
    "# save the dataframe to csv file\n",
    "print('Save the dataframe to csv file:')\n",
    "df.to_csv('sentiments.csv', index=False)\n",
    "\n",
    "print(' ------------------------------------ ')\n",
    "\n",
    "print(' ------------------------------------ ')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load sentiments dataset\n",
    "print('Load sentiments dataset:')\n",
    "df = pd.read_csv('sentiments.csv')\n",
    "\n",
    "# Sentiment Analysis\n",
    "print('Sentiment Analysis:')\n",
    "print('=' * 50)\n",
    "\n",
    "# Take a small sample of the data\n",
    "print('Take a small sample of the data:')\n",
    "df_sample = df.sample(n=1000, random_state=42)\n",
    "\n",
    "# TF\n",
    "print('TF Text Vectorizer:')\n",
    "tf_vectorizer = CountVectorizer(max_features=1000)\n",
    "tf_vectorizer.fit(df_sample['title'])\n",
    "X_text = tf_vectorizer.transform(df_sample['title'])\n",
    "\n",
    "# Include the sentiment scores\n",
    "print('Including the sentiment scores:')\n",
    "X_sentiment = df_sample['sentiment'].values.reshape(-1, 1)\n",
    "X = np.concatenate((X_text.toarray(), X_sentiment), axis=1)\n",
    "\n",
    "# Prepare data for sentiment analysis\n",
    "print('Prepare data for sentiment analysis:')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df_sample['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "print('Train the model:')\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels\n",
    "print('Predict the labels:')\n",
    "y_pred_binary = model.predict(X_test)\n",
    "print('Print the predicted labels:')\n",
    "print(y_pred_binary)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Evaluate the model:')\n",
    "print('Accuracy:')\n",
    "print(accuracy_score(y_test, y_pred_binary))\n",
    "print('Precision, Recall, F1-Score:')\n",
    "print(precision_recall_fscore_support(y_test, y_pred_binary, average='macro'))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred_binary))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred_binary))\n",
    "print('Mean Squared Error:')\n",
    "print(mean_squared_error(y_test, y_pred_binary))\n",
    "print('Mean Absolute Error:')\n",
    "print(mean_absolute_error(y_test, y_pred_binary))\n",
    "print('R2 Score:')\n",
    "print(r2_score(y_test, y_pred_binary))\n",
    "print('Explained Variance Score:')\n",
    "print(explained_variance_score(y_test, y_pred_binary))\n",
    "\n",
    "# Save classification report to csv file\n",
    "print('Save classification report to csv file:')\n",
    "report = classification_report(y_test, y_pred_binary, output_dict=True)\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "df_report.to_csv('classification_reportSent.csv', index=False)\n",
    "\n",
    "# Save confusion matrix to csv file\n",
    "print('Save confusion matrix to csv file:')\n",
    "df_confusion_matrix = pd.DataFrame(confusion_matrix(y_test, y_pred_binary))\n",
    "df_confusion_matrix.to_csv('confusion_matrixSent.csv', index=False)\n",
    "\n",
    "print(' ------------------------------------ ')\n",
    "\n",
    "'''\n",
    "READ ME\n",
    "metadata.csv is the dataset that is used for this project.\n",
    "The dataset is taken from the following link:\n",
    "https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge\n",
    "\n",
    "The dataset is a collection of research papers related to COVID-19, SARS-CoV-2, and related coronaviruses.\n",
    "\n",
    "The dataset contains the following columns:\n",
    "cord_uid, sha, source_x, title, doi, pmcid, pubmed_id, license, abstract, publish_time, authors, journal,\n",
    "mag_id, who_covidence_id, arxiv_id, pdf_json_files, pmc_json_files, url, s2_id\n",
    "\n",
    "\n",
    "'''\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T21:53:12.133646Z",
     "start_time": "2023-06-05T21:49:24.315313Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
